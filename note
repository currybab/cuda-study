# 함수 실행 공간 지정 키워드
- __host__ : host에서 호출, host에서 실행
- __device__ : device에서 호출, device에서 실행
- __global__ : host에서 호출되지만 device에서 실행

# 디바이스 메모리 할당
cudaError_t cudaMalloc(void ** ptr, size_t size);

# 디바이스 메모리 해제
cudaError_t cudaFree(void * ptr);

# 디바이스 메모리 초기화
cudaError_t cudaMemset(void * ptr, int value, size_t size);

# 에러 코드 확인
__host__ __device__ const char* cudaGetErrorName (cudaError_t error);

# 디바이스 메모리 사용량 확인
cudaError_t cudaMemGetInfo(size_t * free, size_t * total);

# 장치간 데이터 복사
cudaError_t cudaMemcpy(void * dst, const void * src, size_t size, enum cudaMemcpyKind kind);

- cudaMemcpyHostToHost: 호스트 메모리 -> 호스트 메모리
- cudaMemcpyHostToDevice: 호스트 메모리 -> 디바이스 메모리
- cudaMemcpyDeviceToHost: 디바이스 메모리 -> 호스트 메모리
- cudaMemcpyDeviceToDevice: 디바이스 메모리 -> 디바이스 메모리
- cudaMemcpyDefault: dst와 src의 포인터 값에 의해 결정 (unified virtual addressing을 지원하는 시스템에서만 사용 가능)

- 2차원, 3차원 데이터 복사를 돕는 cudaMemcpy2D(), cudaMemcpy3D()
- 비동기 복사를 돕는 cudaMemcpyAsync(), cudaMemcpy2DAsync(), cudaMemcpy3DAsync()

# 커널 함수 호출 동기화
cudaError_t cudaDeviceSynchronize();

# 스레드 레이아웃
- 워프 : 32개의 스레드
- 블록이 가지는 최대 스레드 수 : 1024

# gpu 정보 확인
cudaError_t cudaGetDeviceProperties(cudaDeviceProp* prop, int deviceID)
- name : GPU 이름
- major : GPU의 메이저 버전
- minor : GPU의 마이너 버전
- multiProcessorCount : GPU의 멀티 프로세서(SM) 수
- totalGlobalMem : GPU의 총 메모리 크기

# 시스템 내부 gpu의 개수
cudaError_t cudaGetDeviceCount(int * count)

# SM당 CUDA 코어의 갯수
int _ConvertSMVer2Cores(int major, int minor)


# 스레드 수준 메모리
- 레지스터
    - 커널 내부에서 선언된 지역 변수(local variable)를 위해 사용
    - SM내부에 있어서 in-core memory 라고도 부름
    - 레지스터에 대한 접근은 일반적으로 한 GPU cycle보다 작은 시간이 소요
    - 스레드 블록내 모든 스레드들은 블록 내부의 레지스터들을 나누어서 사용.
- 지역 메모리
    - SM 밖에 있는 off-chip memory,  접근 속도는 레지스터보다 느리지만 사용 가능한 메모리 공간이 큼.
    - 물리적으로는 GPU의 디바이스 메모리(DRAM 영역) 공간 일부가 지역 메모리로 사용됨.(400~600 GPU cycle)
    - 레지스터를 사용하기에는 큰 구조체나 배열등이 지역 메모리 공간을 사용.
    - 일반 변수도 레지스터 공간을 할당 받지 못하면 사용(컴파일러에 의해 결정)
    - 스레드당 512KB라는 제한이 있지만, 스레드 하나가 지역 변수를 위해 사용하기에는 충분한 양임.

# 블록 수준 메모리
- 블록 내 모든 스레드들이 접근할 수 있는 공유 메모리 공간.
- 물리적으로도 공유 메모리는 SM 내부에 자리 잡음. 공유 메모리의 속도는 1~4 GPU cycles 정도.
- 크기는 디바이스 메모리보다 작으며, compute capability에 따라 SM당 16~96KB의 크기를 가짐.
- 어떻게 활용하는지에 따라 CUDA 알고리즘의 성능이 크게 달라짐.
- 블록 내 모든 스레드가 공유할 수 있다는 점에서, 블록 내 스레드들 사이 데이터 공유 통로라는 의미도 가짐.
- 모든 블록 내 모든 스레드가 공통으로, 자주 사용하는 데이터를 보관함으로써 메모리 공간을 절약함과 동시에 데이터 접근 속도를 높일 수 있음.
- 하나의 스레드 블록에서 사용하는 공유 메모리 공간의 크기는 활성 블록의 수에 영향을 줌.
- 사용자 관리 캐시 형태로 사용 하는 법
    - 사용자가 직접 관리, __shared__ 키워드를 통해 명시적으로 선언 및 할당받아 사용함.
    - 정적 할당(static allocation): 커널 내부에서 공유 메모리 공간을 선언 및 할당하는 방법
        - 각 스레드의 지역변수가 아닌 스레드 블록 내 모든 스레드가 공유하는 변수로 선언됨.
        - CUDA 프로그램이 컴파일 될 때 그 크기가 결정됨.
    - 동적 할당(dynamic allocation)
        - GPU의 compute capability에 따라 공유 메모리 크기가 다르며, 프로그램의 상황에 따라 공유 메모리의 크기 조절이 필요할 때
        - 크기가 정해지지 않은 extern 배열 형태(빈 대괄호, [] 사용)로 커널 밖에서 선언,
          `extern __shared__ int sharedPool[];`
        - sharedMemory 같은 배열의 크기는 실행 구성의 세번째 인자값에 의해 결정되어, 커널 실행시 메모리 공간이 할당됨.
        - 실행 구성의 세번쨰 인자에는 하나의 값만 전달되므로, 이는 여러개의 공유 메모리 공간을 동적 할당할 수 없다는 의미도 됨.
        - 하나의 커널 안에서 여러 개의 공유 메모리 배열이 필요하다면 
          모든 배열의 크기를 더한 만큼의 공간을 가지는 하나의 큰 공유 메모리 배열을 선언하고, 
          포인터를 이용하여 해당 공간을 분할 하는 방법을 사용해야 함.

# 그리드 수준 메모리
- 그리드 내 모든 스레드가 접근할 수 있는 메모리 영역을 말함.
- 전역 메모리, 상수 메모리, 텍스처 메모리가 있으며, 모두 디바이스 메모리 공간을 사용.
- 전역 메모리는 읽기/쓰기가 모두 자유로움
- 상수 메모리, 텍스처 메모리는 읽기만 가능함, 특수 목적 메모리이며 온-칩 캐시를 활용함.
- 전역 메모리(global memory)
    - 대략적으로는 전역 메모리를 디바이스 메모리라고도 말할 수 있음.
    - GPU 메모리중 수~수십GB의 가장 큰 메모리 공간을 가지지만 접근 속도는 400~800 GPU cycle 정도로 가장 느림.
    - CUDA 프로그램을 위한 데이터는 기본적으로 이 전역 메모리 공간에 적재되며, 필요에 따라 공유 메모리나 지역 메모리로 복사되어 사용.
    - 호스트에서 접근 가능한 GPU 메모리로 호스트-디바이스간 통신 통로이기도 함.
- 상수 메모리(constant memory)
    - 쓰기 연산은 불가능한 읽기 전용 메모리, 디바이스 메모리 영역을 사용하며 최대 크기는 64KB.
    - 64KB라는 작은 크기를 따로 구분해 놓은 이유는 전용 온-칩 메모리인 상수 캐시(constant cache)를 사용하기 때문.
    - 상수 캐시는 compute capability에 따라 다르며 대략 48KB 수준임.
    - 상수 메모리에 대한 접근은 캐싱 되며, 캐싱 적중률이 높은 경우 전역 메모리 사용 대비 데이터 접근 속도를 크게 높일 수 있음.
    - 상수 메모리를 __constant__ 키워드를 통해  전역 범위로 선언, 호스트에 의해 커널 호출 전에 초기화 되어야함.
    - _cudaError_t cudaMemcpyToSymbol (const void* symbol, const void* src,
                        size_t size, size_t offset = 0, cudaMemcpyKind kind = cudayMemcpyHostToDevice)
- 텍스쳐 메모리(texture memory)
    - GPU의 본래 기능인 그래픽스 연산을 위해 사용되는 메모리 공간.
    - 상수 메모리와 달리 2차원 공간적 지역성에 최적화 되어 있음
    - floating-point interpolation 과 같은 다양한 하드웨어 필터링을 지원

# GPU 캐시
- 일정 규칙에 따라 하드웨어가 자동으로 활용하는 공간으로, hardware managed memory라고도 부름
- 연산 장치와 얼마나 가까운지에 따라 L1부터 L2, L3 캐시로 구분됨
- L2캐시는 모든 스트리밍 프로세서들이 공유하는 캐시 공간
- L1캐시는 각 SM마다 할당된 캐시 공간
- cpu의 캐시와 동작 방식은 유사하나 일반적으로 하나의 스레드가 데이터 접근을 요청하는 cpu와 달리,
  gpu는 32개 쓰레드(warp)가 동시에 메모리 접근을 요청한다는 차이점이 있음.
- L1 캐시는 SM 내부의 온-칩 메모리인 공유 메모리 공간을 사용함.
    - 블록 수준 메모리로 이야기했던 공유 메모리와 같은 메모리 공간을 사용함.
    - 각 커널이 SM 내부 온칩메모리를 l1캐시와 스레드 블록을 위한 공유 메모리에 어느 정도 사용할지 조정할 수 있음.
    - cudaFuncSetCacheConfig(kernel, cacheConfig)
        - cudaFuncCachePreferNone : no preference(자동으로 결정)
        - cudaFuncCachePreferShared : shared memory is 48KB
        - cudaFuncCachePreferEqual : shared memory is 32KB
        - cudaFuncCachePreferL1 : shared memory is 16KB

# cuda 메모리 요약
- 공유 범위에 따른 구분
    - 개별 스레드에서만 접근 가능한 스레드 수준 (register, local memory)
    - 블록내 모든 스레드가 접근 가능한 블록 수준 (shared memory)
    - 그리드내 모든 스레드가 접근 가능한 그리드 수준 (global memory, constant memory, texture memory)

- 메모리 공간에 따른 구분
    - 디바이스 메모리 사용: local memory, global memory (400~800 gpu cycles)  (vs l2 cache)
    - sm 내부에 위치: shared memory (1~4 gpu cycles)  (vs l1 cahce)
    - 전용캐시 공간 사용: constant memory, texture memory
    - register (<= 1 gpu cycle)

# 메모리 관점에서 CUDA 프로그램 성능 조율(tuning)

## 병렬성 최대화
- cuda 프로그램에서 병렬성(parallelism)이란 동시에 연산을 수행하는 스레드의 수로 정의할 수 있음.
- 커널 및 블록 또는 스레드 레이아웃을 설계할 때 CUDA 메모리 모델을 고려해야 함.
- 활성 워프와 활성 블록의 수가 많을수록 병렬성이 높아짐.
- 각 SM 내부에는 레지스터들의 집합인 레지스터 파일이 있으며, 해당 SM에 할당된 블록 내 모든 스레드들이 레지스터 파일을 나누어서 사용.
- 한 블록이 필요로 하는 레지스터의 수 = (블록 내 스레드 수) * (한 스레드가 필요로 하는 레지스터의 수)
- 따라서 한 스레드에서 사용하는 레지스터의 수가 많거나 블록 내 스레드가 많은 경우 블록 내 일부 스레드는 레지스터 공간을 할당받지 못할 수 있음.
- 활성 워프
    - 워프 내 모든 스레드들이 필요한 레지스터 공간을 모두 할당받은 워프.
    - 활성 워프의 수가 많더라도 SM 내 CUDA 코어 수에 따라 일부 워프만 실제로 CUDA 코어를 할당받아 연산을 수행하게 됨.
    - CUDA 코어를 사용중인 워프가 입출력 작업 등에 의해 사용중인 CUDA 코어를 반납하면 다른 활성 워프가 CUDA 코어를 할당받아 연산 진행.
    - 이때 워프 사이에 문맥 교환(context switch)이 발생.
    - CPU와 달리 CUDA에서 활성 워프의 경우 필요한 레지스터 공간을 이미 보유한 상태로 무비용 문맥교환(zero context switch overhead)
    - 비활성 워프의 경우 cpu와 같이 높은 문맥 교환 비용 발생. 
    - 따라서 활성 워프가 적은 경우 높은 문맥 교환 비용에 따라 cuda 코어 활용 효율이 떨어짐.
    - 활성 워프를 늘리는 방법
        - 스레드당 사용하는 레지스터의 수를 줄이는 것
            - nvcc 컴파일러 옵션을 통해 지정. `--maxrregcount amount(최대 스레드 수)`
        - 블록내의 스레들르 줄이는 것
        - 항상 더 높은 성능을 보장하지는 않으므로 조율이 필요.

- 활성 블록
    - 필요한 메모리 자원을 모두 할당받은 블록을 의미 (레지스터 & 공유 메모리 공간)
    - 워프 사이와 마찬가지로, 활성 블록들 사이의 문맥교환은 무비용, 비활성 블록에 대한 전환에는 높은 비용이 발생
    - 활성 블록을 늘리는 것이 병렬성과 gpu 활용 효율을 높이는데 중요한 역할을 함.
    - 레지스터 수 조절과 함께 공유 메모리 사용량을 줄이는 것으로 활성 블록을 늘릴 수 있음(튜닝 필요)

- 활성 워프 비율
    - 점유(occupancy)라는 개념을 통해 레지스터와 공유 메모리 사용량에 따른 커널 성능의 변화를 대략적인 예측 및 조절 가능
    - `Occupancy = (# of active warps) / (# of maximum warps)`
    - SM의 최대 워프수 대비 활성 워프 비율. 높은 occupancy는 높은 병렬성을 의미.
    - cuda occupancy calculator 찾아서 다운받아서 사용.

# 블록 내 스레들에 대한 동기화
__syncthreads()
블록 내 모든 스레드가 해당 지점에 도착할 때까지 다른 스레드들의 진행을 막음

# 사용자 관리 캐시
- 알고리즘의 특성을 기반으로 데이터 접근 패턴을 파악하고, 그에 따라 사용자가 직접 목표 데이터를 공유 메모리 영역으로 
  load하거나 내리면서(write-back) 공유 메모리를 캐시처럼 사용하는 것
- 적절히 사용할시 HW 관리 캐시로 사용하는 경우보다 높은 성능을 얻을 수 있음. 커널 성능을 최적화 하는 주요 기법 중 하나.
- 전략의 핵심은 블록 내 스레드들이 자주 접근하는 데이터를 공유 메모리에 가져다 놓음으로써, 
  전역 메모리(디바이스 메모리 영역) 접근 수를 줄이는 것

# 단일 스레드 블록을 사용하는 행렬의 곱셈
- 입력 행렬 A: ROW_SIZE * K_SIZE, B: K_SIZE * COL_SIZE, 출력 행렬 C: ROW_SIZE * COL_SIZE
- 문제 확인 (공유 메모리 사용하지 않을 경우)
    - 입력행렬 A와 B의 원소들은 각각 COL_SIZE와 ROW_SIZE번씩 접근됨.
    - 출력행렬 C의 원소들은 결과를 쓰기 위해 한번만 접근하면됨.
    - 전역 메모리에 대한 접근 횟수는 2 * (ROW_SIZE * COL_SIZE * K_SIZE) + ROW_SIZE * COL_SIZE로
      O(ROW_SIZE * COL_SIZE * K_SIZE)로 추정됨.
- 행렬 A와 B의 운소들은 반복해서 접근하기에 공유 메모리에 올려 놓으면 전역 메모리 접근횟수를 줄일 수 있음.
    - 이경우 전역 메모리에 대한 접근 횟수는 ROW_SIZE * K_SIZE + COL_SIZE * K_SIZE + ROW_SIZE * COL_SIZE로
      O((ROW_SIZE + COL_SIZE) * K_SIZE)로 추정됨.
    - 공유 메모리를 사용하지 않는 경우 대비 접근횟수를 1/ROW_SIZE 또는 1/COL_SIZE 수준으로 크게 감소.
- __shared__ 키워드를 사용해서 정적 할당 방식으로 메모리 공간 할당
- 전역 메모리에서 공유 메모리로 데이터를 복사
    - 대표 스레드가 전담 (비효율적)
    - 행렬 B는 col을 기준으로 데이터를 복사하며, 각 열에서 하나의 스레드만 데이터 복사에 참여,
      행렬 A는 row를 기준으로 데이터를 복사하며, 각 행에서 하나의 스레드만 데이터 복사에 참여
- 오류를 방지하기 위해 모든 스레드를 동기화 작업이 필요 __syncthreads()
- 공유 메모리 미 사용시보다 약 2.5배 높은 성능.

# 공유 메모리를 활용한 행렬 곱셈 프로그램
- 가장 간단한 방법은 gpu에게 공유 메모리 활용을 맡기는 것
- 두번쨰 방법은 전체 데이터가 아닌 일부 데이터만 올려가며 사용하는 것
    - 어떤 데이터를 어떤 시점에 공유 메모리에 올리고 내릴지를 개발자가 결정해야함.
    - 데이터 접근 패턴에 대한 이해가 필요
- m과 n은 1024 이상, k는 512 이상인 크기가 큰 행렬을 다룸.
    - 행렬의 크기가 공유 메모리보다 20~80배 커지므로 행렬 전체를 올릴 수는 없다.
- 전략 1: 행렬 A의 일부 행과 행렬 B의 일부 열을 공유 메모리에 적재
    - 스레드 블록의 크기가 행렬 및 공유 메모리 크기에 의해 제한됨. 
      그 결과로 병렬성이 제한될 수 있어 높은 gpu 활용 효율을 기대하기 어려움
    - 대상 행렬이 매우 큰 경우(행렬 한 줄의 크기 > 공유 메모리 크기)에는 공유 메모리를 사용할 수 없음.
- 전략 2: 행과 열을 블록 단위로 분할하여 공유 메모리에 적재
    - 곱셈과정에서 주목할 점은 같은 p번째 원소인 A(i,p)와 B(p,j)만 특정 시점에 공유 메모리에 있으면 된다는 점.
    - 따라서 p의 방향을 중심으로 행과 열을 분할하고, p의 진행에 따라 분할된 데이터 블록을 공유 메모리로 가져와 사용하는 전략을 사용할 수 있음.
    



    