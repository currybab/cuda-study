# 함수 실행 공간 지정 키워드
- __host__ : host에서 호출, host에서 실행
- __device__ : device에서 호출, device에서 실행
- __global__ : host에서 호출되지만 device에서 실행

# 디바이스 메모리 할당
cudaError_t cudaMalloc(void ** ptr, size_t size);

# 디바이스 메모리 해제
cudaError_t cudaFree(void * ptr);

# 디바이스 메모리 초기화
cudaError_t cudaMemset(void * ptr, int value, size_t size);

# 에러 코드 확인
__host__ __device__ const char* cudaGetErrorName (cudaError_t error);

# 디바이스 메모리 사용량 확인
cudaError_t cudaMemGetInfo(size_t * free, size_t * total);

# 장치간 데이터 복사
cudaError_t cudaMemcpy(void * dst, const void * src, size_t size, enum cudaMemcpyKind kind);

- cudaMemcpyHostToHost: 호스트 메모리 -> 호스트 메모리
- cudaMemcpyHostToDevice: 호스트 메모리 -> 디바이스 메모리
- cudaMemcpyDeviceToHost: 디바이스 메모리 -> 호스트 메모리
- cudaMemcpyDeviceToDevice: 디바이스 메모리 -> 디바이스 메모리
- cudaMemcpyDefault: dst와 src의 포인터 값에 의해 결정 (unified virtual addressing을 지원하는 시스템에서만 사용 가능)

- 2차원, 3차원 데이터 복사를 돕는 cudaMemcpy2D(), cudaMemcpy3D()
- 비동기 복사를 돕는 cudaMemcpyAsync(), cudaMemcpy2DAsync(), cudaMemcpy3DAsync()

# 커널 함수 호출 동기화
cudaError_t cudaDeviceSynchronize();

# 스레드 레이아웃
- 워프 : 32개의 스레드
- 블록이 가지는 최대 스레드 수 : 1024
- 워프 < 쓰레드 블록 < 그리드

# gpu 정보 확인
cudaError_t cudaGetDeviceProperties(cudaDeviceProp* prop, int deviceID)
- name : GPU 이름
- major : GPU의 메이저 버전
- minor : GPU의 마이너 버전
- multiProcessorCount : GPU의 멀티 프로세서(SM) 수
- totalGlobalMem : GPU의 총 메모리 크기

# 시스템 내부 gpu의 개수
cudaError_t cudaGetDeviceCount(int * count)

# SM당 CUDA 코어의 갯수
int _ConvertSMVer2Cores(int major, int minor)


# 스레드 수준 메모리
- 레지스터
    - 커널 내부에서 선언된 지역 변수(local variable)를 위해 사용
    - SM내부에 있어서 in-core memory 라고도 부름
    - 레지스터에 대한 접근은 일반적으로 한 GPU cycle보다 작은 시간이 소요
    - 스레드 블록내 모든 스레드들은 블록 내부의 레지스터들을 나누어서 사용.
- 지역 메모리
    - SM 밖에 있는 off-chip memory,  접근 속도는 레지스터보다 느리지만 사용 가능한 메모리 공간이 큼.
    - 물리적으로는 GPU의 디바이스 메모리(DRAM 영역) 공간 일부가 지역 메모리로 사용됨.(400~600 GPU cycle)
    - 레지스터를 사용하기에는 큰 구조체나 배열등이 지역 메모리 공간을 사용.
    - 일반 변수도 레지스터 공간을 할당 받지 못하면 사용(컴파일러에 의해 결정)
    - 스레드당 512KB라는 제한이 있지만, 스레드 하나가 지역 변수를 위해 사용하기에는 충분한 양임.

# 블록 수준 메모리
- 블록 내 모든 스레드들이 접근할 수 있는 공유 메모리 공간.
- 물리적으로도 공유 메모리는 SM 내부에 자리 잡음. 공유 메모리의 속도는 1~4 GPU cycles 정도.
- 크기는 디바이스 메모리보다 작으며, compute capability에 따라 SM당 16~96KB의 크기를 가짐.
- 어떻게 활용하는지에 따라 CUDA 알고리즘의 성능이 크게 달라짐.
- 블록 내 모든 스레드가 공유할 수 있다는 점에서, 블록 내 스레드들 사이 데이터 공유 통로라는 의미도 가짐.
- 모든 블록 내 모든 스레드가 공통으로, 자주 사용하는 데이터를 보관함으로써 메모리 공간을 절약함과 동시에 데이터 접근 속도를 높일 수 있음.
- 하나의 스레드 블록에서 사용하는 공유 메모리 공간의 크기는 활성 블록의 수에 영향을 줌.
- 사용자 관리 캐시 형태로 사용 하는 법
    - 사용자가 직접 관리, __shared__ 키워드를 통해 명시적으로 선언 및 할당받아 사용함.
    - 정적 할당(static allocation): 커널 내부에서 공유 메모리 공간을 선언 및 할당하는 방법
        - 각 스레드의 지역변수가 아닌 스레드 블록 내 모든 스레드가 공유하는 변수로 선언됨.
        - CUDA 프로그램이 컴파일 될 때 그 크기가 결정됨.
    - 동적 할당(dynamic allocation)
        - GPU의 compute capability에 따라 공유 메모리 크기가 다르며, 프로그램의 상황에 따라 공유 메모리의 크기 조절이 필요할 때
        - 크기가 정해지지 않은 extern 배열 형태(빈 대괄호, [] 사용)로 커널 밖에서 선언,
          `extern __shared__ int sharedPool[];`
        - sharedMemory 같은 배열의 크기는 실행 구성의 세번째 인자값에 의해 결정되어, 커널 실행시 메모리 공간이 할당됨.
        - 실행 구성의 세번쨰 인자에는 하나의 값만 전달되므로, 이는 여러개의 공유 메모리 공간을 동적 할당할 수 없다는 의미도 됨.
        - 하나의 커널 안에서 여러 개의 공유 메모리 배열이 필요하다면 
          모든 배열의 크기를 더한 만큼의 공간을 가지는 하나의 큰 공유 메모리 배열을 선언하고, 
          포인터를 이용하여 해당 공간을 분할 하는 방법을 사용해야 함.

# 그리드 수준 메모리
- 그리드 내 모든 스레드가 접근할 수 있는 메모리 영역을 말함.
- 전역 메모리, 상수 메모리, 텍스처 메모리가 있으며, 모두 디바이스 메모리 공간을 사용.
- 전역 메모리는 읽기/쓰기가 모두 자유로움
- 상수 메모리, 텍스처 메모리는 읽기만 가능함, 특수 목적 메모리이며 온-칩 캐시를 활용함.
- 전역 메모리(global memory)
    - 대략적으로는 전역 메모리를 디바이스 메모리라고도 말할 수 있음.
    - GPU 메모리중 수~수십GB의 가장 큰 메모리 공간을 가지지만 접근 속도는 400~800 GPU cycle 정도로 가장 느림.
    - CUDA 프로그램을 위한 데이터는 기본적으로 이 전역 메모리 공간에 적재되며, 필요에 따라 공유 메모리나 지역 메모리로 복사되어 사용.
    - 호스트에서 접근 가능한 GPU 메모리로 호스트-디바이스간 통신 통로이기도 함.
- 상수 메모리(constant memory)
    - 쓰기 연산은 불가능한 읽기 전용 메모리, 디바이스 메모리 영역을 사용하며 최대 크기는 64KB.
    - 64KB라는 작은 크기를 따로 구분해 놓은 이유는 전용 온-칩 메모리인 상수 캐시(constant cache)를 사용하기 때문.
    - 상수 캐시는 compute capability에 따라 다르며 대략 48KB 수준임.
    - 상수 메모리에 대한 접근은 캐싱 되며, 캐싱 적중률이 높은 경우 전역 메모리 사용 대비 데이터 접근 속도를 크게 높일 수 있음.
    - 상수 메모리를 __constant__ 키워드를 통해  전역 범위로 선언, 호스트에 의해 커널 호출 전에 초기화 되어야함.
    - _cudaError_t cudaMemcpyToSymbol (const void* symbol, const void* src,
                        size_t size, size_t offset = 0, cudaMemcpyKind kind = cudayMemcpyHostToDevice)
- 텍스쳐 메모리(texture memory)
    - GPU의 본래 기능인 그래픽스 연산을 위해 사용되는 메모리 공간.
    - 상수 메모리와 달리 2차원 공간적 지역성에 최적화 되어 있음
    - floating-point interpolation 과 같은 다양한 하드웨어 필터링을 지원

# GPU 캐시
- 일정 규칙에 따라 하드웨어가 자동으로 활용하는 공간으로, hardware managed memory라고도 부름
- 연산 장치와 얼마나 가까운지에 따라 L1부터 L2, L3 캐시로 구분됨
- L2캐시는 모든 스트리밍 프로세서들이 공유하는 캐시 공간
- L1캐시는 각 SM마다 할당된 캐시 공간
- cpu의 캐시와 동작 방식은 유사하나 일반적으로 하나의 스레드가 데이터 접근을 요청하는 cpu와 달리,
  gpu는 32개 쓰레드(warp)가 동시에 메모리 접근을 요청한다는 차이점이 있음.
- L1 캐시는 SM 내부의 온-칩 메모리인 공유 메모리 공간을 사용함.
    - 블록 수준 메모리로 이야기했던 공유 메모리와 같은 메모리 공간을 사용함.
    - 각 커널이 SM 내부 온칩메모리를 l1캐시와 스레드 블록을 위한 공유 메모리에 어느 정도 사용할지 조정할 수 있음.
    - cudaFuncSetCacheConfig(kernel, cacheConfig)
        - cudaFuncCachePreferNone : no preference(자동으로 결정)
        - cudaFuncCachePreferShared : shared memory is 48KB
        - cudaFuncCachePreferEqual : shared memory is 32KB
        - cudaFuncCachePreferL1 : shared memory is 16KB

# cuda 메모리 요약
- 공유 범위에 따른 구분
    - 개별 스레드에서만 접근 가능한 스레드 수준 (register, local memory)
    - 블록내 모든 스레드가 접근 가능한 블록 수준 (shared memory)
    - 그리드내 모든 스레드가 접근 가능한 그리드 수준 (global memory, constant memory, texture memory)

- 메모리 공간에 따른 구분
    - 디바이스 메모리 사용: local memory, global memory (400~800 gpu cycles)
    - sm 내부 SRAM에 위치: shared memory (1~4 gpu cycles)  (vs l1 cahce)
    - 전용캐시 공간 사용: constant memory, texture memory
    - register (<= 1 gpu cycle)
    - gpu 다이 내부의 SRAM에 위치: l2 cache

# 메모리 관점에서 CUDA 프로그램 성능 조율(tuning)

## 병렬성 최대화
- cuda 프로그램에서 병렬성(parallelism)이란 동시에 연산을 수행하는 스레드의 수로 정의할 수 있음.
- 커널 및 블록 또는 스레드 레이아웃을 설계할 때 CUDA 메모리 모델을 고려해야 함.
- 활성 워프와 활성 블록의 수가 많을수록 병렬성이 높아짐.
- 각 SM 내부에는 레지스터들의 집합인 레지스터 파일이 있으며, 해당 SM에 할당된 블록 내 모든 스레드들이 레지스터 파일을 나누어서 사용.
- 한 블록이 필요로 하는 레지스터의 수 = (블록 내 스레드 수) * (한 스레드가 필요로 하는 레지스터의 수)
- 따라서 한 스레드에서 사용하는 레지스터의 수가 많거나 블록 내 스레드가 많은 경우 블록 내 일부 스레드는 레지스터 공간을 할당받지 못할 수 있음.
- 활성 워프
    - 워프 내 모든 스레드들이 필요한 레지스터 공간을 모두 할당받은 워프.
    - 활성 워프의 수가 많더라도 SM 내 CUDA 코어 수에 따라 일부 워프만 실제로 CUDA 코어를 할당받아 연산을 수행하게 됨.
    - CUDA 코어를 사용중인 워프가 입출력 작업 등에 의해 사용중인 CUDA 코어를 반납하면 다른 활성 워프가 CUDA 코어를 할당받아 연산 진행.
    - 이때 워프 사이에 문맥 교환(context switch)이 발생.
    - CPU와 달리 CUDA에서 활성 워프의 경우 필요한 레지스터 공간을 이미 보유한 상태로 무비용 문맥교환(zero context switch overhead)
    - 비활성 워프의 경우 cpu와 같이 높은 문맥 교환 비용 발생. 
    - 따라서 활성 워프가 적은 경우 높은 문맥 교환 비용에 따라 cuda 코어 활용 효율이 떨어짐.
    - 활성 워프를 늘리는 방법
        - 스레드당 사용하는 레지스터의 수를 줄이는 것
            - nvcc 컴파일러 옵션을 통해 지정. `--maxrregcount amount(최대 스레드 수)`
        - 블록내의 스레들르 줄이는 것
        - 항상 더 높은 성능을 보장하지는 않으므로 조율이 필요.

- 활성 블록
    - 필요한 메모리 자원을 모두 할당받은 블록을 의미 (레지스터 & 공유 메모리 공간)
    - 워프 사이와 마찬가지로, 활성 블록들 사이의 문맥교환은 무비용, 비활성 블록에 대한 전환에는 높은 비용이 발생
    - 활성 블록을 늘리는 것이 병렬성과 gpu 활용 효율을 높이는데 중요한 역할을 함.
    - 레지스터 수 조절과 함께 공유 메모리 사용량을 줄이는 것으로 활성 블록을 늘릴 수 있음(튜닝 필요)

- 활성 워프 비율
    - 점유(occupancy)라는 개념을 통해 레지스터와 공유 메모리 사용량에 따른 커널 성능의 변화를 대략적인 예측 및 조절 가능
    - `Occupancy = (# of active warps) / (# of maximum warps)`
    - SM의 최대 워프수 대비 활성 워프 비율. 높은 occupancy는 높은 병렬성을 의미.
    - cuda occupancy calculator 찾아서 다운받아서 사용.

# 블록 내 스레들에 대한 동기화
__syncthreads()
블록 내 모든 스레드가 해당 지점에 도착할 때까지 다른 스레드들의 진행을 막음

# 사용자 관리 캐시
- 알고리즘의 특성을 기반으로 데이터 접근 패턴을 파악하고, 그에 따라 사용자가 직접 목표 데이터를 공유 메모리 영역으로 
  load하거나 내리면서(write-back) 공유 메모리를 캐시처럼 사용하는 것
- 적절히 사용할시 HW 관리 캐시로 사용하는 경우보다 높은 성능을 얻을 수 있음. 커널 성능을 최적화 하는 주요 기법 중 하나.
- 전략의 핵심은 블록 내 스레드들이 자주 접근하는 데이터를 공유 메모리에 가져다 놓음으로써, 
  전역 메모리(디바이스 메모리 영역) 접근 수를 줄이는 것

# 단일 스레드 블록을 사용하는 행렬의 곱셈
- 입력 행렬 A: ROW_SIZE * K_SIZE, B: K_SIZE * COL_SIZE, 출력 행렬 C: ROW_SIZE * COL_SIZE
- 문제 확인 (공유 메모리 사용하지 않을 경우)
    - 입력행렬 A와 B의 원소들은 각각 COL_SIZE와 ROW_SIZE번씩 접근됨.
    - 출력행렬 C의 원소들은 결과를 쓰기 위해 한번만 접근하면됨.
    - 전역 메모리에 대한 접근 횟수는 2 * (ROW_SIZE * COL_SIZE * K_SIZE) + ROW_SIZE * COL_SIZE로
      O(ROW_SIZE * COL_SIZE * K_SIZE)로 추정됨.
- 행렬 A와 B의 운소들은 반복해서 접근하기에 공유 메모리에 올려 놓으면 전역 메모리 접근횟수를 줄일 수 있음.
    - 이경우 전역 메모리에 대한 접근 횟수는 ROW_SIZE * K_SIZE + COL_SIZE * K_SIZE + ROW_SIZE * COL_SIZE로
      O((ROW_SIZE + COL_SIZE) * K_SIZE)로 추정됨.
    - 공유 메모리를 사용하지 않는 경우 대비 접근횟수를 1/ROW_SIZE 또는 1/COL_SIZE 수준으로 크게 감소.
- __shared__ 키워드를 사용해서 정적 할당 방식으로 메모리 공간 할당
- 전역 메모리에서 공유 메모리로 데이터를 복사
    - 대표 스레드가 전담 (비효율적)
    - 행렬 B는 col을 기준으로 데이터를 복사하며, 각 열에서 하나의 스레드만 데이터 복사에 참여,
      행렬 A는 row를 기준으로 데이터를 복사하며, 각 행에서 하나의 스레드만 데이터 복사에 참여
- 오류를 방지하기 위해 모든 스레드를 동기화 작업이 필요 __syncthreads()
- 공유 메모리 미 사용시보다 약 2.5배 높은 성능.

# 공유 메모리를 활용한 행렬 곱셈 프로그램
- 가장 간단한 방법은 gpu에게 공유 메모리 활용을 맡기는 것
- 두번쨰 방법은 전체 데이터가 아닌 일부 데이터만 올려가며 사용하는 것
    - 어떤 데이터를 어떤 시점에 공유 메모리에 올리고 내릴지를 개발자가 결정해야함.
    - 데이터 접근 패턴에 대한 이해가 필요
- m과 n은 1024 이상, k는 512 이상인 크기가 큰 행렬을 다룸.
    - 행렬의 크기가 공유 메모리보다 20~80배 커지므로 행렬 전체를 올릴 수는 없다.
- 전략 1: 행렬 A의 일부 행과 행렬 B의 일부 열을 공유 메모리에 적재
    - 스레드 블록의 크기가 행렬 및 공유 메모리 크기에 의해 제한됨. 
      그 결과로 병렬성이 제한될 수 있어 높은 gpu 활용 효율을 기대하기 어려움
    - 대상 행렬이 매우 큰 경우(행렬 한 줄의 크기 > 공유 메모리 크기)에는 공유 메모리를 사용할 수 없음.
- 전략 2: 행과 열을 블록 단위로 분할하여 공유 메모리에 적재
    - 곱셈과정에서 주목할 점은 같은 p번째 원소인 A(i,p)와 B(p,j)만 특정 시점에 공유 메모리에 있으면 된다는 점.
    - 따라서 p의 방향을 중심으로 행과 열을 분할하고, p의 진행에 따라 분할된 데이터 블록을 공유 메모리로 가져와 사용하는 전략을 사용할 수 있음.
    
# 전역 메모리 접근 최적화
- L2 캐시에 대한 기본 전송(transaction) 단위, 즉 cache line의 크기는 32byte
- L1 캐시의 캐시라인 크기는 128byte
- 정렬된 메모리 접근: 요청한 데이터의 시작접이 캐시에 대응되는 데이터 블록의 시작 지점(경계)과 일치하는 경우
- 병합된 메모리 접근: 32개 스레드가 연속적인 메모리 공간의 데이터에 접근하는것.
- 정렬 및 병합된 메모리 접근은 워프의 메모리 접근 요청을 최소한의 메모리 전송으로 처리할 수 있어,
  높은 메모리 대역폭을 달성함으로써 커널 성능을 높일 수 있음.(이론적 차이 최대 32배)
- CUDA 커널을 작성할 때는 정렬 및 병합된 메모리 접근 패턴을 갖도록 설계하는 것이 좋음.
- 행렬곱의 경우 (행,열)->(x,y)로 접근하는 경우 (y,x)로 접근하는 것에 비해 메모리 접근 횟수가 많아짐
- 구조체의 배열(AoS) vs 배열의 구조체(SoA)
    - SoA가 병합된 메모리 접근을 유도하기에 커널을 작성할때 더 권장됨.
    - 하지만 각 알고리즘마다 알맞은 데이터 접근 패턴이 다를 수 있으므로 원리를 이해하고 사용하는 것이 좋음

# 공유 메모리 접근 최적화
- 메모리 뱅크
    - 메모리에 대한 접근을 관리하는 모듈
    - 공유 메모리는 32개의 메모리 뱅크로 구성되어 있으며, 32개의 영역으로 나누어져있다는 의미 (워프 내 스레드 갯수와 관련)
    - 구성단위는 8바이트로 이상적인 경우 32개 스레드가 8바이트 데이터에 동시에 접근할 수 있음.
    - 뱅크는 서로 독립적으로 작동될 수 있음.
- 뱅크 충돌(bank conflict)
    - 하나의 뱅크에 여러 스레드가 동시에 접근하려는 경우
    - 뱅크 충돌이 발생하면 해당 뱅크 영역에 대한 접근이 직렬화되어 커널 성능 저하로 이어짐.
- 행렬곱셈(x,y) 구조에서 공유 메모리에 데이터 저장시 전치를 함으로써 뱅크 충돌을 최소화할 수 있음.

# 동기화
- 둘 이상의 연산 주체가 서로 정보를 교환하는 행위
- 특정 정보를 공유하는 것과 서로 실행 순서를 맞추는 것으로 할 수 있음.
    - 장벽(barrier): 모든 스레드가 특정 지점에 도착할 때까지 다른 스레드들의 진행을 막음
    - 상호 배제(mutual exclusion): 특정 영역의 작업을 한번에 하나의 스레드만 수행하도록 함.
- 동기화 함수: 장벽 역할을 수행하며, 동기화 범위에 따라 다른 함수 제공
    - 블록 수준 동기화 : __syncthreads()
    - 워프 수준 동기화 : __syncwarp()
    - 그리드(커널) 수준 동기화 : 따로 제공 되지 않음. 호출 및 종료 시점에 암묵적 동기화가 수행. 커널 분리로 구현해야함.
- 동기화는 커널의 성능을 떨어뜨리는 주요 병목 지접이기 때문에 동기화 수준에 따른 적절한 선택이 중요.
- 원자 함수: 상호 배제 역할을 수행함. 한번에 하나의 스레드만 해당 데이터에 접근할 수 있도록 보장.
    - 한 스레드가 특정 데이터에 원자함수로 접근하는 동안에는 다른 스레드는 접근 불가
    - read-modify-write를 한번의 연산으로 수행하며 전역 메모리 및 공유 메모리에 있는 데이터에 적용할 수 있음.
    - 산술 함수와 비트 단위 함수로 나뉨.
- 총 실행된 스레드 카운트 예시
    - 전체 스레드에 대해 원자함수 하나로(10배 이상 느려짐)
    - 각 스레드 블록에 대해 __syncthreads()로 atomicAdd를 수행하고 grid level에서 atomicAdd를 수행
    - 각 워프에 대해 하는것도 생각해 볼 수 있음. 그래도 꽤 빠름.
- 수동 제어(mutual control)
    - 수동 동기화 제어를 위해 사용할 수 있는 대표적인 도구 세가지는 스레드 번호, 원자 함수, 동기화 함수
    - 필요하면 spin lock while(조건); 사용
- 불필요한 동기화 사용을 자제해야 하며, 동기화를 최소화하도록 알고리즘을 설계해야함.
  동기화 범위(동기화에 참여하는 스레드의 수)를 최소화하는 것도 중요.

# CUDA 스트림
- 호스트와 디바이스 사이의 명령 전달 통로.
- NULL 스트림: 암묵적으로 선언된 스트림, 사용할 스트림을 명시하지 않았을 때 사용되는 기본 스트림(디바이스당 하나만 존재)
- Non-NULL 스트림: 사용자가 명시적으로 생성 및 사용하는 스트림, 사용자 필요에 따라 여러개 생성해서 사용 가능
- 호스트에서 하나의 스트림을 통해 전달하는 명령들은 순서대로 스트림에 쌓이며, 디바이스는 먼저 들어온 명령대로 하나씩 순차 처리
- 서로 다른 스트림 사이의 실행 순서는 비결정적임.
- 또한 서로 다른 스트림은 비동기적으로 동작함. 
- 디바이스의 성능 및 상태에 따라 둘 이상의 스트림에 있는 명령이 동시에 처리될 수 있음.
- 비동기적 실행 특성은 디바이스의 동시 실행 능력과 함께 CUDA 프로그램의 성능을 높이기 위해 활용될 수 있음.
- Non-NULL 스트림 생성 및 제거
    - 생성: cudaError_t cudaStreamCreate(cudaStream_t *stream)
    - 제거: cudaError_t cudaStreamDestroy(cudaStream_t stream)
    - 스트림 생성 후 커널 호출 시 스트림을 네번째 인자로 전달, 중간에 사용하지 않는 인자가 있더라도 0으로 지정해줄 것
- 각 연산 장치의 연산 작업들과 호스트와 디바이스 사이의 데이터 통신은 비동기적으로 실행 가능함
    - 호스트 연산과 디바이스 연산
    - 호스트 연산과 호스트-디바이스 사이 데이터 통신
    - 디바이스 연산과 호스트-디바이스 사이 데이터 통신
    - 호스트->디바이스 데이터 복사와 디바이스->호스트 데이터 복사(디바이스에 따라 다름)
    - 서로 다른 디바이스들의 연산(디바이스가 둘 이상인 경우)
- 비동기적 수행특성과 동시 실행능력 활용
    - 데이터 전송과 디바이스의 연산을 중첩(overlapping)
        - 벡터의 합과 같이 처리할 데이터들이 서로 독립적일 때, 일부 데이터가 준비되면 전체 데이터 복사를 기다리지 않고 연산
        - 커널 실행과 데이터 통신의 중첩은 여러 개의 Non-NULL 스트림을 사용하고,
          중첩할 작업들을 서로 다른 스트림에 넣어주는 방법으로 구현할 수 있음.
- 동기적으로 수행되는 cuda api중 대표적인 것이 데이터 복사 api인 cudaMemcpy(),
  따라서 연산과 데이터 중첩 전략을 사용하기 위해서는 비동기적으로 수행되는 데이터 복사 api를 사용해야함.
    cudaError_t cudaMemcpyAsync(void* dst, const void* src, size_t size, 
                            enum cudaMemcpyKind, cudaStream_t stream = 0);
    - 마지막 인자로 명령을 전달할 스트림을 지정할 수 있음
    - 해당 인자를 주지 않으면 기본 설정에 따라 NULL 스트림으로 설정됨.
    - NULL스트림은 동기적으로 동작하므로 비동기적 데이터 복사를 하기 위해서는 반드시 Non-NULL 스트림을 사용해야함.
    - 데이터 복사와 관련된 호스트 측 메모리 영역이 핀드 메모리(pinned memory)로 설정되어야 함.
    - 핀드 메모리: 물리 메모리(DRAM)에 상주하는 메모리 영역, 즉 교체될 수 없는(page-locked) 메모리
    - 동기적일 때는 데이터 통신이 끝날때까지 대기하기 떄문에 문제가 없음, 하지만 비동기에서는 호스트 입장에서
      작업이 끝난것으로 판단하기 때문에 페이지 교체가능한 상태가 됨.
    - 동일한 맥락에서 해당 함수를 사용하더라도 대상 호스트 메모리가 핀드 메모리가 아니라면 데이터 통신은 동기적으로 작동함.
- 핀드 메모리 할당 및 해제
    - 할당: cudaError_t cudaMallocHost(void** ptr, size_t size)
    - 해제: cudaError_t cudaFreeHost(void* ptr)
- stream 동기화
    - 모든 스트림을 동기화: cudaError_t cudaDeviceSynchronize()
    - 인자로 전달한 스트림에 대한 동기화: cudaError_t cudaStreamSynchronize(cudaStream_t stream)
    - 스트림의 현재 상태 확인: cudaError_t cudaStreamQuery(cudaStream_t stream)
        - 스트림 내 모든 작업이 완료된 경우 cudaSuccess(=0) 반환
        - 남은 작업이 있는 경우 cudaErrorNotReady(=600) 반환
    - 암묵적 동기화 발생하는 경우
        - 호스트에서 핀드 메모리 할당시 - cudaMallocHost
        - 디바이스 메모리 할당 시 - cudaMalloc
        - 디바이스 메모리의 값을 초기화 시 - cudaMemset
        - 같은 메모리 위치에 데이터를 복사하려 할 때
        - NULL 스트림에 있는 명령이 수행될 때
        - L1캐시/공유 메모리 설정 변경이 발생할 시

# CUDA 이벤트
- 스트림의 상태 확인 및 동기화를 위해 사용할 수 있는 CUDA api
- 스트림내 명령들 사이에 끼워 놓을 수 있는 일종의 표식(marker)로 생각할 수 있음.
- 이벤트 생성 및 제거
    - 생성 api를 통해 명시적으로 생성해서 사용하며, 사용 후에는 제거해주어야 함.
    - 생성: cudaError_t cudaEventCreate(cudaEvent_t *event)
    - 제거: cudaError_t cudaEventDestroy(cudaEvent_t event)
- 이벤트 기록: 생성한 cuda 이벤트를 스트림에 넣는 것
    - cudaError_t cudaEventRecord(cudaEvent_t event, cudaStream_t stream = 0)
- 이벤트 동기화: 호스트를 특정 cuda 이벤트가 발생할 때까지 대기하게 하는 것
    - cudaError_t cudaEventSynchronize(cudaEvent_t event)
- 이벤트 발생 여부 확인: cudaError_t cudaEventquery(cudaEvent_t event)
- 이벤트 사이 소요시간 측정: 
    cudaError_t cudaEventElapsedTime(float* ms, cudaEvent_t start, cudaEvent_t stop)
- CUDA 이벤트는 스트림 내 작업 흐름에 따라 정확한 시점에 발생하기 때문에, 호스트 코드 대비 더 정확한 시간 측정 가능한

# 다중 GPU 사용
- 시스템 내 GPU 개수 확인: cudaError_t cudaGetDeviceCount(int *count)
- 각 GPU에 할당된 번호 확인: cudaError_t cudaGetDeviceProperties(cudaDeviceProp* prop, int deviceID)
- 사용할 GPU 선택: cudaError_t cudaSetDevice(int deviceID)
- 특정 코드 지점에서 대상 GPU 확인: cudaError_t cudaGetDevice(int *deviceID)
